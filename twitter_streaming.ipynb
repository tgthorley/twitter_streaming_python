{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic stuff - don't worry to much about it I'm just loading all the things you need\n",
    "# If you have any problems running the first cell run `pip install -r requirements.txt`\n",
    "\n",
    "%matplotlib inline\n",
    "import tweepy\n",
    "import pandas as pd\n",
    "import json\n",
    "import numbers\n",
    "import re\n",
    "import os.path\n",
    "import datetime\n",
    "\n",
    "pd.options.display.max_colwidth = 400\n",
    "pd.options.display.max_rows = 25\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Twitter app at https://apps.twitter.com. Then generate an access token. put your consumer keys and access tokens in the credentials.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This takes the credentials that you just created and format them so I can use them to access twitter and get tweets\n",
    "def load_credentials():\n",
    "    consumer_key, consumer_key_secret, access_token, access_token_secret = (None,)*4\n",
    "    if not os.path.isfile('credentials.txt'): \n",
    "        return consumer_key, consumer_key_secret, access_token, access_token_secret\n",
    "    lines = [line.rstrip('\\n') for line in open('credentials.txt')]\n",
    "    chars_to_strip = \" \\'\\\"\"\n",
    "    for line in lines:\n",
    "        if \"consumer_key\" in line and 'fill_in' not in line:\n",
    "            consumer_key = re.findall(r'[\\\"\\']([^\\\"\\']*)[\\\"\\']', line)[0]\n",
    "        if \"consumer_secret\" in line and 'fill_in' not in line:\n",
    "            consumer_key_secret = re.findall(r'[\\\"\\']([^\\\"\\']*)[\\\"\\']', line)[0]\n",
    "        if \"access_token\" in line and 'fill_in' not in line:\n",
    "            access_token = re.findall(r'[\\\"\\']([^\\\"\\']*)[\\\"\\']', line)[0]\n",
    "        if \"access_secret\" in line and 'fill_in' not in line:\n",
    "            access_token_secret = re.findall(r'[\\\"\\']([^\\\"\\']*)[\\\"\\']', line)[0]\n",
    "    return consumer_key, consumer_key_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Testing to make sure that it works. \n",
    "# This is pulling 20 tweets from your own Twitter timeline (tweets from ppl you follow).\n",
    "# If tweets print out below, then your credentials are all set!\n",
    "\n",
    "tweets_raw_data = []\n",
    "public_tweets = api.home_timeline()\n",
    "for tweet in public_tweets:\n",
    "    print tweet.text\n",
    "    tweets_raw_data.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is what the data you get back actually looks like (just the first tweet)\n",
    "tweets_raw_data[0]._json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The meat of the project  - this is a function that gets tweets from twitter based on your search criteria\n",
    "# It does some basic cleaning up and puts the tweets in a format that is easier to work with for stats stuff\n",
    "\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def __init__(self,limit=20,print_output=True,save_output=True,\n",
    "                 filename='file.csv',include_rts=True,strict_text_search=False,\n",
    "                 search_terms=None):\n",
    "        self.df = pd.DataFrame()\n",
    "        self.limit = limit\n",
    "        self.counter = 0\n",
    "        self.print_output = print_output\n",
    "        self.header=False\n",
    "        self.save_output=save_output\n",
    "        self.filename=filename\n",
    "        self.include_rts=include_rts\n",
    "        self.strict_text_search = strict_text_search\n",
    "        self.search_terms = search_terms\n",
    "\n",
    "    def on_data(self, data):\n",
    "        d = {}\n",
    "        decoded = json.loads(data)\n",
    "        tweet_fields_to_collect = ['created_at','id','text','source','favorite_count','coordinates','lang','place','retweet_count','retweeted','truncated']\n",
    "        user_fields_to_collect = ['name','screen_name','location','id_str','statuses_count','followers_count','friends_count','favourites_count','description']\n",
    "        if self.strict_text_search:\n",
    "            if not isinstance(self.search_terms, list):\n",
    "                self.search_terms = re.findall(r\"[\\w']+\", self.search_terms)\n",
    "            if not any(term.lower() in decoded['text'].lower() for term in self.search_terms):\n",
    "                print \"skipped\"\n",
    "                print decoded['text']\n",
    "                return True\n",
    "        for k,v in decoded.iteritems():\n",
    "            if k in tweet_fields_to_collect:\n",
    "                if isinstance(v, numbers.Number):\n",
    "                    v = str(v)\n",
    "                try:\n",
    "                    d['tweet_' + k.strip()] = v\n",
    "                except:\n",
    "                    print \"Failure collecting tweet field\", v.encode('ascii', 'ignore')\n",
    "            if k=='user':\n",
    "                for user_k,user_v in v.iteritems():\n",
    "                    if user_k in user_fields_to_collect:\n",
    "                        if isinstance(user_v, numbers.Number):\n",
    "                            user_v = str(user_v)\n",
    "                        try:\n",
    "                            d[user_k.strip()]=user_v\n",
    "                        except:\n",
    "                            print \"Failure collecting user field\",user_v.encode('ascii', 'ignore')\n",
    "            if k=='retweeted_status':\n",
    "                for retweet_k,retweet_v in v.iteritems():\n",
    "                    if retweet_k in tweet_fields_to_collect:\n",
    "                        if isinstance(retweet_v, numbers.Number):\n",
    "                            retweet_v = str(retweet_v)\n",
    "                        try:\n",
    "                            d['retweet_'+retweet_k.strip()]=retweet_v\n",
    "                        except:\n",
    "                            print \"Failure collecting retweet field\",user_v.encode('ascii', 'ignore')\n",
    "        if not self.include_rts:\n",
    "            if ('retweet_text' in d and len(d['retweet_text'])>0) or d['tweet_text'].startswith('RT @'):\n",
    "                return True\n",
    "        tweet_df = pd.DataFrame(d, index=[0])\n",
    "        frames = [self.df, tweet_df]\n",
    "        self.df = pd.concat(frames)\n",
    "        self.counter+=1\n",
    "        if self.print_output:\n",
    "            try:\n",
    "                print(decoded['text'])\n",
    "            except:\n",
    "                print(\"Failure outputting tweet text\",decoded['text'].encode('ascii', 'ignore'))\n",
    "        if self.counter>=self.limit:\n",
    "            print(\"finished collecting %s tweets, ending\" % self.limit)\n",
    "            if self.include_rts and 'retweet_text' in self.df.columns:\n",
    "                self.df = self.df[['tweet_' + x for x in tweet_fields_to_collect] + user_fields_to_collect + ['retweet_' + x for x in tweet_fields_to_collect]]\n",
    "            else:\n",
    "                self.df = self.df[['tweet_' + x for x in tweet_fields_to_collect] + user_fields_to_collect]\n",
    "            self.df.rename(columns={'id_str':'user_id'},inplace=True)\n",
    "            self.df.to_csv(self.filename, index=False, encoding='utf-8')\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            return False\n",
    "        \n",
    "    def on_disconnect(self, notice):\n",
    "        print(\"disconnecting due to \" + str(notice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## This is where you put what you want to search for - i.e. BREXIT\n",
    "search_query = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is you actually getting the tweets and saving them to a file\n",
    "\n",
    "# so where you need to alter stuff to match what you need\n",
    "# Options you can set:\n",
    "#        limit: int, how many tweets to capture\n",
    "#        print_output: bool, whether to print the tweet to screen\n",
    "#        save_output: bool, whether to save the tweet data to a csv file\n",
    "#        filename: str, the filename to name the saved output, by default it's file.csv\n",
    "#        include_rts: bool, whether to capture retweets\n",
    "#        strict_text_search: bool, ocasionally, stream will capture a tweet that doesn't actually include the search query\n",
    "#            set to True to filter out these \"accidental\" tweets\n",
    "#        search_terms: str or array, pass in the search query or an array of terms you want to use for filtering\n",
    "#           if strict_text_search = True. Script checks and turns any string into array of strings\n",
    "\n",
    "filename = '%s_%s.csv' % (search_query, datetime.datetime.now().strftime(\"%Y-%m-%d_%H.%M.%S\"))\n",
    "myStreamListener = MyStreamListener(limit=50,\n",
    "                                    print_output=False,\n",
    "                                    filename=filename,\n",
    "                                    search_terms=search_query,\n",
    "                                    strict_text_search=True,\n",
    "                                    include_rts=False)\n",
    "myStream = tweepy.Stream(auth, listener=myStreamListener)\n",
    "myStream.filter(track=[search_query])\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is how you display what you have\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is how you display just the tweets\n",
    "df[df['tweet_text']][['tweet_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is some basic stats as an example\n",
    "# They show if there is a correlation between number of tweets and number of statuses for users who have less than 1000 followers. \n",
    "# (Spoiler alert - yes there is but there is also a lot of noise)\n",
    "\n",
    "import statsmodels.formula.api as sm\n",
    "import seaborn as sns\n",
    "result = sm.ols(formula=\"statuses_count ~ followers_count\", data=df.query(\"followers_count<1000\")).fit()\n",
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is a pretty graph showing what the stats mean\n",
    "# A straight diagonal line low left to high right means correlation\n",
    "ax = sns.regplot(x=\"followers_count\", y=\"statuses_count\", data=df.query('followers_count<1000'))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
